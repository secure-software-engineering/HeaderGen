{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97958ffb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "source_hidden": true
   },
   "source": [
    "### Index of ML Operations<a id='top_phases'></a>\n",
    "<div><ul>\n",
    "<ul><li><details><summary style='list-style: none; cursor: pointer;'><strong>Imported Libraries</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li><b>numpy</b></li>\n",
    "<li><b>pandas</b></li>\n",
    "<li><b>sklearn</b></li>\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Visualization</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none;'><h3><span style='color:#42a5f5'>Data Preparation</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Profiling and Exploratory Data Analysis</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Cleaning Filtering</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Data Sub-sampling and Train-test Splitting</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><h3><span style='color:#42a5f5'>Feature Engineering</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Feature Engineering\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.TfidfVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to :class:`CountVectorizer` followed by\n",
    ":class:`TfidfTransformer`.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (string transformation) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word or character n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
    "        is first read from the file and then passed to the given callable\n",
    "        analyzer.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "    list is returned. 'english' is currently the only supported string\n",
    "    value.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
    "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
    "    only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "max_df : float or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "    documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
    "    of documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non-zero term counts are set to 1. This does not mean\n",
    "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "    is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "\n",
    "dtype : dtype, default=float64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "norm : {'l1', 'l2'} or None, default='l2'\n",
    "    Each output row will have unit norm, either:\n",
    "\n",
    "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "      similarity between two vectors is their dot product when l2 norm has\n",
    "      been applied.\n",
    "    - 'l1': Sum of absolute values of vector elements is 1.\n",
    "      See :func:`preprocessing.normalize`.\n",
    "    - None: No normalization.\n",
    "\n",
    "use_idf : bool, default=True\n",
    "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "\n",
    "smooth_idf : bool, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : bool, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "idf_ : array of shape (n_features,)\n",
    "    The inverse document frequency (IDF) vector; only defined\n",
    "    if ``use_idf`` is True.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
    "    matrix of counts.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = TfidfVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.shape)\n",
    "(4, 9)\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 7</u></strong></summary><small><a href=#7>goto cell # 7</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 10</u></strong></summary><small><a href=#10>goto cell # 10</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 14</u></strong></summary><small><a href=#14>goto cell # 14</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 18</u></strong></summary><small><a href=#18>goto cell # 18</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 21</u></strong></summary><small><a href=#21>goto cell # 21</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.TfidfVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to :class:`CountVectorizer` followed by\n",
    ":class:`TfidfTransformer`.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (string transformation) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word or character n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
    "        is first read from the file and then passed to the given callable\n",
    "        analyzer.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "    list is returned. 'english' is currently the only supported string\n",
    "    value.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
    "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
    "    only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "max_df : float or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "    documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
    "    of documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non-zero term counts are set to 1. This does not mean\n",
    "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "    is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "\n",
    "dtype : dtype, default=float64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "norm : {'l1', 'l2'} or None, default='l2'\n",
    "    Each output row will have unit norm, either:\n",
    "\n",
    "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "      similarity between two vectors is their dot product when l2 norm has\n",
    "      been applied.\n",
    "    - 'l1': Sum of absolute values of vector elements is 1.\n",
    "      See :func:`preprocessing.normalize`.\n",
    "    - None: No normalization.\n",
    "\n",
    "use_idf : bool, default=True\n",
    "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "\n",
    "smooth_idf : bool, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : bool, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "idf_ : array of shape (n_features,)\n",
    "    The inverse document frequency (IDF) vector; only defined\n",
    "    if ``use_idf`` is True.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
    "    matrix of counts.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = TfidfVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.shape)\n",
    "(4, 9)\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 24</u></strong></summary><small><a href=#24>goto cell # 24</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 25</u></strong></summary><small><a href=#25>goto cell # 25</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Feature Transformation</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Feature Selection</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><h3><span style='color:#42a5f5'>Model Building and Training</span></h3></summary>\n",
    "<ul>\n",
    "\n",
    "<li><details><summary style='list-style: none; cursor: pointer;'><u>View All \"Model Building and Training\" Calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 7</u></strong></summary><small><a href=#7>goto cell # 7</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 10</u></strong></summary><small><a href=#10>goto cell # 10</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 14</u></strong></summary><small><a href=#14>goto cell # 14</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 18</u></strong></summary><small><a href=#18>goto cell # 18</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 21</u></strong></summary><small><a href=#21>goto cell # 21</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<li><details open><summary style='list-style: none; cursor: pointer;'><strong><u>Cell # 25</u></strong></summary><small><a href=#25>goto cell # 25</a></small>\n",
    "<ul>\n",
    "\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u> | (No Args Found) </summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "\n",
    "</ul>\n",
    "</details></li>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Model Training</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Model Parameter Tuning</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "<ul><li><details><summary style='list-style: none;'><s>Model Validation and Assembling</s> (no calls found)</summary>\n",
    "<ul>\n",
    "\n",
    "None\n",
    "\n",
    "</ul>\n",
    "</details></li></ul>\n",
    "</ul>\n",
    "<hr>\n",
    "\n",
    "<details><summary style='list-style: none; cursor: pointer;'><strong>View All ML API Calls in Notebook</strong></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <b>len</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>len.len</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Return the number of items in a container.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li> <b>numpy</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>numpy</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "NumPy\n",
    "=====\n",
    "\n",
    "Provides\n",
    "  1. An array object of arbitrary homogeneous items\n",
    "  2. Fast mathematical operations over arrays\n",
    "  3. Linear Algebra, Fourier Transforms, Random Number Generation\n",
    "\n",
    "How to use the documentation\n",
    "----------------------------\n",
    "Documentation is available in two forms: docstrings provided\n",
    "with the code, and a loose standing reference guide, available from\n",
    "`the NumPy homepage <https://numpy.org>`_.\n",
    "\n",
    "We recommend exploring the docstrings using\n",
    "`IPython <https://ipython.org>`_, an advanced Python shell with\n",
    "TAB-completion and introspection capabilities.  See below for further\n",
    "instructions.\n",
    "\n",
    "The docstring examples assume that `numpy` has been imported as `np`::\n",
    "\n",
    "  >>> import numpy as np\n",
    "\n",
    "Code snippets are indicated by three greater-than signs::\n",
    "\n",
    "  >>> x = 42\n",
    "  >>> x = x + 1\n",
    "\n",
    "Use the built-in ``help`` function to view a function's docstring::\n",
    "\n",
    "  >>> help(np.sort)\n",
    "  ... # doctest: +SKIP\n",
    "\n",
    "For some objects, ``np.info(obj)`` may provide additional help.  This is\n",
    "particularly true if you see the line \"Help on ufunc object:\" at the top\n",
    "of the help() page.  Ufuncs are implemented in C, not Python, for speed.\n",
    "The native Python help() does not know how to view their help, but our\n",
    "np.info() function does.\n",
    "\n",
    "To search for documents containing a keyword, do::\n",
    "\n",
    "  >>> np.lookfor('keyword')\n",
    "  ... # doctest: +SKIP\n",
    "\n",
    "General-purpose documents like a glossary and help on the basic concepts\n",
    "of numpy are available under the ``doc`` sub-module::\n",
    "\n",
    "  >>> from numpy import doc\n",
    "  >>> help(doc)\n",
    "  ... # doctest: +SKIP\n",
    "\n",
    "Available subpackages\n",
    "---------------------\n",
    "lib\n",
    "    Basic functions used by several sub-packages.\n",
    "random\n",
    "    Core Random Tools\n",
    "linalg\n",
    "    Core Linear Algebra Tools\n",
    "fft\n",
    "    Core FFT routines\n",
    "polynomial\n",
    "    Polynomial tools\n",
    "testing\n",
    "    NumPy testing tools\n",
    "distutils\n",
    "    Enhancements to distutils with support for\n",
    "    Fortran compilers support and more.\n",
    "\n",
    "Utilities\n",
    "---------\n",
    "test\n",
    "    Run numpy unittests\n",
    "show_config\n",
    "    Show numpy build configuration\n",
    "dual\n",
    "    Overwrite certain functions with high-performance SciPy tools.\n",
    "    Note: `numpy.dual` is deprecated.  Use the functions from NumPy or Scipy\n",
    "    directly instead of importing them from `numpy.dual`.\n",
    "matlib\n",
    "    Make everything matrices.\n",
    "__version__\n",
    "    NumPy version string\n",
    "\n",
    "Viewing documentation using IPython\n",
    "-----------------------------------\n",
    "Start IPython with the NumPy profile (``ipython -p numpy``), which will\n",
    "import `numpy` under the alias `np`.  Then, use the ``cpaste`` command to\n",
    "paste examples into the shell.  To see which functions are available in\n",
    "`numpy`, type ``np.<TAB>`` (where ``<TAB>`` refers to the TAB key), or use\n",
    "``np.*cos*?<ENTER>`` (where ``<ENTER>`` refers to the ENTER key) to narrow\n",
    "down the list.  To view the docstring for a function, use\n",
    "``np.cos?<ENTER>`` (to view the docstring) and ``np.cos??<ENTER>`` (to view\n",
    "the source code).\n",
    "\n",
    "Copies vs. in-place operation\n",
    "-----------------------------\n",
    "Most of the functions in `numpy` return a copy of the array argument\n",
    "(e.g., `np.sort`).  In-place versions of these functions are often\n",
    "available as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\n",
    "Exceptions to this rule are documented.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li> <b>pandas</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>pandas</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "pandas - a powerful data analysis and manipulation library for Python\n",
    "=====================================================================\n",
    "\n",
    "**pandas** is a Python package providing fast, flexible, and expressive data\n",
    "structures designed to make working with \"relational\" or \"labeled\" data both\n",
    "easy and intuitive. It aims to be the fundamental high-level building block for\n",
    "doing practical, **real world** data analysis in Python. Additionally, it has\n",
    "the broader goal of becoming **the most powerful and flexible open source data\n",
    "analysis / manipulation tool available in any language**. It is already well on\n",
    "its way toward this goal.\n",
    "\n",
    "Main Features\n",
    "-------------\n",
    "Here are just a few of the things that pandas does well:\n",
    "\n",
    "  - Easy handling of missing data in floating point as well as non-floating\n",
    "    point data.\n",
    "  - Size mutability: columns can be inserted and deleted from DataFrame and\n",
    "    higher dimensional objects\n",
    "  - Automatic and explicit data alignment: objects can be explicitly aligned\n",
    "    to a set of labels, or the user can simply ignore the labels and let\n",
    "    `Series`, `DataFrame`, etc. automatically align the data for you in\n",
    "    computations.\n",
    "  - Powerful, flexible group by functionality to perform split-apply-combine\n",
    "    operations on data sets, for both aggregating and transforming data.\n",
    "  - Make it easy to convert ragged, differently-indexed data in other Python\n",
    "    and NumPy data structures into DataFrame objects.\n",
    "  - Intelligent label-based slicing, fancy indexing, and subsetting of large\n",
    "    data sets.\n",
    "  - Intuitive merging and joining data sets.\n",
    "  - Flexible reshaping and pivoting of data sets.\n",
    "  - Hierarchical labeling of axes (possible to have multiple labels per tick).\n",
    "  - Robust IO tools for loading data from flat files (CSV and delimited),\n",
    "    Excel files, databases, and saving/loading data from the ultrafast HDF5\n",
    "    format.\n",
    "  - Time series-specific functionality: date range generation and frequency\n",
    "    conversion, moving window statistics, date shifting and lagging.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li> <b>sklearn</b>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.TfidfTransformer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Transform a count matrix to a normalized tf or tf-idf representation.\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse\n",
    "document-frequency. This is a common term weighting scheme in information\n",
    "retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a\n",
    "token in a given document is to scale down the impact of tokens that occur\n",
    "very frequently in a given corpus and that are hence empirically less\n",
    "informative than features that occur in a small fraction of the training\n",
    "corpus.\n",
    "\n",
    "The formula that is used to compute the tf-idf for a term t of a document d\n",
    "in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is\n",
    "computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where\n",
    "n is the total number of documents in the document set and df(t) is the\n",
    "document frequency of t; the document frequency is the number of documents\n",
    "in the document set that contain the term t. The effect of adding \"1\" to\n",
    "the idf in the equation above is that terms with zero idf, i.e., terms\n",
    "that occur in all documents in a training set, will not be entirely\n",
    "ignored.\n",
    "(Note that the idf formula above differs from the standard textbook\n",
    "notation that defines the idf as\n",
    "idf(t) = log [ n / (df(t) + 1) ]).\n",
    "\n",
    "If ``smooth_idf=True`` (the default), the constant \"1\" is added to the\n",
    "numerator and denominator of the idf as if an extra document was seen\n",
    "containing every term in the collection exactly once, which prevents\n",
    "zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.\n",
    "\n",
    "Furthermore, the formulas used to compute tf and idf depend\n",
    "on parameter settings that correspond to the SMART notation used in IR\n",
    "as follows:\n",
    "\n",
    "Tf is \"n\" (natural) by default, \"l\" (logarithmic) when\n",
    "``sublinear_tf=True``.\n",
    "Idf is \"t\" when use_idf is given, \"n\" (none) otherwise.\n",
    "Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none)\n",
    "when ``norm=None``.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "norm : {'l1', 'l2'} or None, default='l2'\n",
    "    Each output row will have unit norm, either:\n",
    "\n",
    "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "      similarity between two vectors is their dot product when l2 norm has\n",
    "      been applied.\n",
    "    - 'l1': Sum of absolute values of vector elements is 1.\n",
    "      See :func:`preprocessing.normalize`.\n",
    "    - None: No normalization.\n",
    "\n",
    "use_idf : bool, default=True\n",
    "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "\n",
    "smooth_idf : bool, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : bool, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "idf_ : array of shape (n_features)\n",
    "    The inverse document frequency (IDF) vector; only defined\n",
    "    if  ``use_idf`` is True.\n",
    "\n",
    "    .. versionadded:: 0.20\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix of\n",
    "    TF-IDF features.\n",
    "\n",
    "HashingVectorizer : Convert a collection of text documents to a matrix\n",
    "    of token occurrences.\n",
    "\n",
    "References\n",
    "----------\n",
    ".. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern\n",
    "               Information Retrieval. Addison Wesley, pp. 68-74.\n",
    "\n",
    ".. [MRS2008] C.D. Manning, P. Raghavan and H. Schtze  (2008).\n",
    "               Introduction to Information Retrieval. Cambridge University\n",
    "               Press, pp. 118-120.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import TfidfTransformer\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> from sklearn.pipeline import Pipeline\n",
    ">>> corpus = ['this is the first document',\n",
    "...           'this document is the second document',\n",
    "...           'and this is the third one',\n",
    "...           'is this the first document']\n",
    ">>> vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "...               'and', 'one']\n",
    ">>> pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "...                  ('tfid', TfidfTransformer())]).fit(corpus)\n",
    ">>> pipe['count'].transform(corpus).toarray()\n",
    "array([[1, 1, 1, 1, 0, 1, 0, 0],\n",
    "       [1, 2, 0, 1, 1, 1, 0, 0],\n",
    "       [1, 0, 0, 1, 0, 1, 1, 1],\n",
    "       [1, 1, 1, 1, 0, 1, 0, 0]])\n",
    ">>> pipe['tfid'].idf_\n",
    "array([1.        , 1.22314355, 1.51082562, 1.        , 1.91629073,\n",
    "       1.        , 1.91629073, 1.91629073])\n",
    ">>> pipe.transform(corpus).shape\n",
    "(4, 8)\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.TfidfVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to :class:`CountVectorizer` followed by\n",
    ":class:`TfidfTransformer`.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (string transformation) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word or character n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
    "        is first read from the file and then passed to the given callable\n",
    "        analyzer.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "    list is returned. 'english' is currently the only supported string\n",
    "    value.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
    "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
    "    only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "max_df : float or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "    documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
    "    of documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non-zero term counts are set to 1. This does not mean\n",
    "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "    is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "\n",
    "dtype : dtype, default=float64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "norm : {'l1', 'l2'} or None, default='l2'\n",
    "    Each output row will have unit norm, either:\n",
    "\n",
    "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "      similarity between two vectors is their dot product when l2 norm has\n",
    "      been applied.\n",
    "    - 'l1': Sum of absolute values of vector elements is 1.\n",
    "      See :func:`preprocessing.normalize`.\n",
    "    - None: No normalization.\n",
    "\n",
    "use_idf : bool, default=True\n",
    "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "\n",
    "smooth_idf : bool, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : bool, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "idf_ : array of shape (n_features,)\n",
    "    The inverse document frequency (IDF) vector; only defined\n",
    "    if ``use_idf`` is True.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
    "    matrix of counts.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = TfidfVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.shape)\n",
    "(4, 9)\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._stochastic_gradient.SGDClassifier</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "\n",
    "This estimator implements regularized linear models with stochastic\n",
    "gradient descent (SGD) learning: the gradient of the loss is estimated\n",
    "each sample at a time and the model is updated along the way with a\n",
    "decreasing strength schedule (aka learning rate). SGD allows minibatch\n",
    "(online/out-of-core) learning via the `partial_fit` method.\n",
    "For best results using the default learning rate schedule, the data should\n",
    "have zero mean and unit variance.\n",
    "\n",
    "This implementation works with data represented as dense or sparse arrays\n",
    "of floating point values for the features. The model it fits can be\n",
    "controlled with the loss parameter; by default, it fits a linear support\n",
    "vector machine (SVM).\n",
    "\n",
    "The regularizer is a penalty added to the loss function that shrinks model\n",
    "parameters towards the zero vector using either the squared euclidean norm\n",
    "L2 or the absolute norm L1 or a combination of both (Elastic Net). If the\n",
    "parameter update crosses the 0.0 value because of the regularizer, the\n",
    "update is truncated to 0.0 to allow for learning sparse models and achieve\n",
    "online feature selection.\n",
    "\n",
    "Read more in the :ref:`User Guide <sgd>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "loss : {'hinge', 'log_loss', 'log', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge'\n",
    "    The loss function to be used.\n",
    "\n",
    "    - 'hinge' gives a linear SVM.\n",
    "    - 'log_loss' gives logistic regression, a probabilistic classifier.\n",
    "    - 'modified_huber' is another smooth loss that brings tolerance to\n",
    "       outliers as well as probability estimates.\n",
    "    - 'squared_hinge' is like hinge but is quadratically penalized.\n",
    "    - 'perceptron' is the linear loss used by the perceptron algorithm.\n",
    "    - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and\n",
    "      'squared_epsilon_insensitive' are designed for regression but can be useful\n",
    "      in classification as well; see\n",
    "      :class:`~sklearn.linear_model.SGDRegressor` for a description.\n",
    "\n",
    "    More details about the losses formulas can be found in the\n",
    "    :ref:`User Guide <sgd_mathematical_formulation>`.\n",
    "\n",
    "    .. deprecated:: 1.1\n",
    "        The loss 'log' was deprecated in v1.1 and will be removed\n",
    "        in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
    "\n",
    "penalty : {'l2', 'l1', 'elasticnet', None}, default='l2'\n",
    "    The penalty (aka regularization term) to be used. Defaults to 'l2'\n",
    "    which is the standard regularizer for linear SVM models. 'l1' and\n",
    "    'elasticnet' might bring sparsity to the model (feature selection)\n",
    "    not achievable with 'l2'. No penalty is added when set to `None`.\n",
    "\n",
    "alpha : float, default=0.0001\n",
    "    Constant that multiplies the regularization term. The higher the\n",
    "    value, the stronger the regularization.\n",
    "    Also used to compute the learning rate when set to `learning_rate` is\n",
    "    set to 'optimal'.\n",
    "    Values must be in the range `[0.0, inf)`.\n",
    "\n",
    "l1_ratio : float, default=0.15\n",
    "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.\n",
    "    l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
    "    Only used if `penalty` is 'elasticnet'.\n",
    "    Values must be in the range `[0.0, 1.0]`.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Whether the intercept should be estimated or not. If False, the\n",
    "    data is assumed to be already centered.\n",
    "\n",
    "max_iter : int, default=1000\n",
    "    The maximum number of passes over the training data (aka epochs).\n",
    "    It only impacts the behavior in the ``fit`` method, and not the\n",
    "    :meth:`partial_fit` method.\n",
    "    Values must be in the range `[1, inf)`.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "tol : float or None, default=1e-3\n",
    "    The stopping criterion. If it is not None, training will stop\n",
    "    when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive\n",
    "    epochs.\n",
    "    Convergence is checked against the training loss or the\n",
    "    validation loss depending on the `early_stopping` parameter.\n",
    "    Values must be in the range `[0.0, inf)`.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "\n",
    "shuffle : bool, default=True\n",
    "    Whether or not the training data should be shuffled after each epoch.\n",
    "\n",
    "verbose : int, default=0\n",
    "    The verbosity level.\n",
    "    Values must be in the range `[0, inf)`.\n",
    "\n",
    "epsilon : float, default=0.1\n",
    "    Epsilon in the epsilon-insensitive loss functions; only if `loss` is\n",
    "    'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.\n",
    "    For 'huber', determines the threshold at which it becomes less\n",
    "    important to get the prediction exactly right.\n",
    "    For epsilon-insensitive, any differences between the current prediction\n",
    "    and the correct label are ignored if they are less than this threshold.\n",
    "    Values must be in the range `[0.0, inf)`.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    The number of CPUs to use to do the OVA (One Versus All, for\n",
    "    multi-class problems) computation.\n",
    "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "    for more details.\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used for shuffling the data, when ``shuffle`` is set to ``True``.\n",
    "    Pass an int for reproducible output across multiple function calls.\n",
    "    See :term:`Glossary <random_state>`.\n",
    "    Integer values must be in the range `[0, 2**32 - 1]`.\n",
    "\n",
    "learning_rate : str, default='optimal'\n",
    "    The learning rate schedule:\n",
    "\n",
    "    - 'constant': `eta = eta0`\n",
    "    - 'optimal': `eta = 1.0 / (alpha * (t + t0))`\n",
    "      where `t0` is chosen by a heuristic proposed by Leon Bottou.\n",
    "    - 'invscaling': `eta = eta0 / pow(t, power_t)`\n",
    "    - 'adaptive': `eta = eta0`, as long as the training keeps decreasing.\n",
    "      Each time n_iter_no_change consecutive epochs fail to decrease the\n",
    "      training loss by tol or fail to increase validation score by tol if\n",
    "      `early_stopping` is `True`, the current learning rate is divided by 5.\n",
    "\n",
    "        .. versionadded:: 0.20\n",
    "            Added 'adaptive' option\n",
    "\n",
    "eta0 : float, default=0.0\n",
    "    The initial learning rate for the 'constant', 'invscaling' or\n",
    "    'adaptive' schedules. The default value is 0.0 as eta0 is not used by\n",
    "    the default schedule 'optimal'.\n",
    "    Values must be in the range `(0.0, inf)`.\n",
    "\n",
    "power_t : float, default=0.5\n",
    "    The exponent for inverse scaling learning rate [default 0.5].\n",
    "    Values must be in the range `(-inf, inf)`.\n",
    "\n",
    "early_stopping : bool, default=False\n",
    "    Whether to use early stopping to terminate training when validation\n",
    "    score is not improving. If set to `True`, it will automatically set aside\n",
    "    a stratified fraction of training data as validation and terminate\n",
    "    training when validation score returned by the `score` method is not\n",
    "    improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "\n",
    "    .. versionadded:: 0.20\n",
    "        Added 'early_stopping' option\n",
    "\n",
    "validation_fraction : float, default=0.1\n",
    "    The proportion of training data to set aside as validation set for\n",
    "    early stopping. Must be between 0 and 1.\n",
    "    Only used if `early_stopping` is True.\n",
    "    Values must be in the range `(0.0, 1.0)`.\n",
    "\n",
    "    .. versionadded:: 0.20\n",
    "        Added 'validation_fraction' option\n",
    "\n",
    "n_iter_no_change : int, default=5\n",
    "    Number of iterations with no improvement to wait before stopping\n",
    "    fitting.\n",
    "    Convergence is checked against the training loss or the\n",
    "    validation loss depending on the `early_stopping` parameter.\n",
    "    Integer values must be in the range `[1, max_iter)`.\n",
    "\n",
    "    .. versionadded:: 0.20\n",
    "        Added 'n_iter_no_change' option\n",
    "\n",
    "class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
    "    Preset for the class_weight fit parameter.\n",
    "\n",
    "    Weights associated with classes. If not given, all classes\n",
    "    are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    Repeatedly calling fit or partial_fit when warm_start is True can\n",
    "    result in a different solution than when calling fit a single time\n",
    "    because of the way the data is shuffled.\n",
    "    If a dynamic learning rate is used, the learning rate is adapted\n",
    "    depending on the number of samples already seen. Calling ``fit`` resets\n",
    "    this counter, while ``partial_fit`` will result in increasing the\n",
    "    existing counter.\n",
    "\n",
    "average : bool or int, default=False\n",
    "    When set to `True`, computes the averaged SGD weights across all\n",
    "    updates and stores the result in the ``coef_`` attribute. If set to\n",
    "    an int greater than 1, averaging will begin once the total number of\n",
    "    samples seen reaches `average`. So ``average=10`` will begin\n",
    "    averaging after seeing 10 samples.\n",
    "    Integer values must be in the range `[1, n_samples]`.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
    "    Weights assigned to the features.\n",
    "\n",
    "intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
    "    Constants in decision function.\n",
    "\n",
    "n_iter_ : int\n",
    "    The actual number of iterations before reaching the stopping criterion.\n",
    "    For multiclass fits, it is the maximum over every binary fit.\n",
    "\n",
    "loss_function_ : concrete ``LossFunction``\n",
    "\n",
    "classes_ : array of shape (n_classes,)\n",
    "\n",
    "t_ : int\n",
    "    Number of weight updates performed during training.\n",
    "    Same as ``(n_iter_ * n_samples + 1)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "sklearn.svm.LinearSVC : Linear support vector classification.\n",
    "LogisticRegression : Logistic regression.\n",
    "Perceptron : Inherits from SGDClassifier. ``Perceptron()`` is equivalent to\n",
    "    ``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\",\n",
    "    penalty=None)``.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    ">>> Y = np.array([1, 1, 2, 2])\n",
    ">>> # Always scale the input. The most convenient way is to use a pipeline.\n",
    ">>> clf = make_pipeline(StandardScaler(),\n",
    "...                     SGDClassifier(max_iter=1000, tol=1e-3))\n",
    ">>> clf.fit(X, Y)\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('sgdclassifier', SGDClassifier())])\n",
    ">>> print(clf.predict([[-0.8, -1]]))\n",
    "[1]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.Pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Pipeline of transforms with a final estimator.\n",
    "\n",
    "Sequentially apply a list of transforms and a final estimator.\n",
    "Intermediate steps of the pipeline must be 'transforms', that is, they\n",
    "must implement `fit` and `transform` methods.\n",
    "The final estimator only needs to implement `fit`.\n",
    "The transformers in the pipeline can be cached using ``memory`` argument.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be\n",
    "cross-validated together while setting different parameters. For this, it\n",
    "enables setting parameters of the various steps using their names and the\n",
    "parameter name separated by a `'__'`, as in the example below. A step's\n",
    "estimator may be replaced entirely by setting the parameter with its name\n",
    "to another estimator, or a transformer removed by setting it to\n",
    "`'passthrough'` or `None`.\n",
    "\n",
    "Read more in the :ref:`User Guide <pipeline>`.\n",
    "\n",
    ".. versionadded:: 0.5\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "steps : list of tuple\n",
    "    List of (name, transform) tuples (implementing `fit`/`transform`) that\n",
    "    are chained in sequential order. The last transform must be an\n",
    "    estimator.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "named_steps : :class:`~sklearn.utils.Bunch`\n",
    "    Dictionary-like object, with the following attributes.\n",
    "    Read-only attribute to access any step parameter by user given name.\n",
    "    Keys are step names and values are steps parameters.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The classes labels. Only exist if the last step of the pipeline is a\n",
    "    classifier.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`. Only defined if the\n",
    "    underlying first estimator in `steps` exposes such an attribute\n",
    "    when fit.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Only defined if the\n",
    "    underlying estimator exposes such an attribute when fit.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "make_pipeline : Convenience function for simplified pipeline construction.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.svm import SVC\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.datasets import make_classification\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> from sklearn.pipeline import Pipeline\n",
    ">>> X, y = make_classification(random_state=0)\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "...                                                     random_state=0)\n",
    ">>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    ">>> # The pipeline can be used as any other estimator\n",
    ">>> # and avoids leaking the test set into the train set\n",
    ">>> pipe.fit(X_train, y_train)\n",
    "Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\n",
    ">>> pipe.score(X_test, y_test)\n",
    "0.88\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.svm._classes.LinearSVC</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Linear Support Vector Classification.\n",
    "\n",
    "Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
    "liblinear rather than libsvm, so it has more flexibility in the choice of\n",
    "penalties and loss functions and should scale better to large numbers of\n",
    "samples.\n",
    "\n",
    "This class supports both dense and sparse input and the multiclass support\n",
    "is handled according to a one-vs-the-rest scheme.\n",
    "\n",
    "Read more in the :ref:`User Guide <svm_classification>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2'}, default='l2'\n",
    "    Specifies the norm used in the penalization. The 'l2'\n",
    "    penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
    "    vectors that are sparse.\n",
    "\n",
    "loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
    "    Specifies the loss function. 'hinge' is the standard SVM loss\n",
    "    (used e.g. by the SVC class) while 'squared_hinge' is the\n",
    "    square of the hinge loss. The combination of ``penalty='l1'``\n",
    "    and ``loss='hinge'`` is not supported.\n",
    "\n",
    "dual : bool, default=True\n",
    "    Select the algorithm to either solve the dual or primal\n",
    "    optimization problem. Prefer dual=False when n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Regularization parameter. The strength of the regularization is\n",
    "    inversely proportional to C. Must be strictly positive.\n",
    "\n",
    "multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
    "    Determines the multi-class strategy if `y` contains more than\n",
    "    two classes.\n",
    "    ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
    "    ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
    "    While `crammer_singer` is interesting from a theoretical perspective\n",
    "    as it is consistent, it is seldom used in practice as it rarely leads\n",
    "    to better accuracy and is more expensive to compute.\n",
    "    If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
    "    will be ignored.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Whether to calculate the intercept for this model. If set\n",
    "    to false, no intercept will be used in calculations\n",
    "    (i.e. data is expected to be already centered).\n",
    "\n",
    "intercept_scaling : float, default=1.0\n",
    "    When self.fit_intercept is True, instance vector x becomes\n",
    "    ``[x, self.intercept_scaling]``,\n",
    "    i.e. a \"synthetic\" feature with constant value equals to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes intercept_scaling * synthetic feature weight\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Set the parameter C of class i to ``class_weight[i]*C`` for\n",
    "    SVC. If not given, all classes are supposed to have\n",
    "    weight one.\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "verbose : int, default=0\n",
    "    Enable verbose output. Note that this setting takes advantage of a\n",
    "    per-process runtime setting in liblinear that, if enabled, may not work\n",
    "    properly in a multithreaded context.\n",
    "\n",
    "random_state : int, RandomState instance or None, default=None\n",
    "    Controls the pseudo random number generation for shuffling the data for\n",
    "    the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
    "    underlying implementation of :class:`LinearSVC` is not random and\n",
    "    ``random_state`` has no effect on the results.\n",
    "    Pass an int for reproducible output across multiple function calls.\n",
    "    See :term:`Glossary <random_state>`.\n",
    "\n",
    "max_iter : int, default=1000\n",
    "    The maximum number of iterations to be run.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
    "    Weights assigned to the features (coefficients in the primal\n",
    "    problem).\n",
    "\n",
    "    ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
    "    follows the internal memory layout of liblinear.\n",
    "\n",
    "intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
    "    Constants in decision function.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    The unique classes labels.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : int\n",
    "    Maximum number of iterations run across all classes.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
    "    the kernel can be non-linear but its SMO algorithm does not\n",
    "    scale to large number of samples as LinearSVC does.\n",
    "\n",
    "    Furthermore SVC multi-class mode is implemented using one\n",
    "    vs one scheme while LinearSVC uses one vs the rest. It is\n",
    "    possible to implement one vs the rest with SVC by using the\n",
    "    :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
    "\n",
    "    Finally SVC can fit dense data without memory copy if the input\n",
    "    is C-contiguous. Sparse data will still incur memory copy though.\n",
    "\n",
    "sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
    "    cost function as LinearSVC\n",
    "    by adjusting the penalty and loss parameters. In addition it requires\n",
    "    less memory, allows incremental (online) learning, and implements\n",
    "    various loss functions and regularization regimes.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller ``tol`` parameter.\n",
    "\n",
    "The underlying implementation, liblinear, uses a sparse internal\n",
    "representation for the data that will incur a memory copy.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "`LIBLINEAR: A Library for Large Linear Classification\n",
    "<https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.svm import LinearSVC\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.datasets import make_classification\n",
    ">>> X, y = make_classification(n_features=4, random_state=0)\n",
    ">>> clf = make_pipeline(StandardScaler(),\n",
    "...                     LinearSVC(random_state=0, tol=1e-5))\n",
    ">>> clf.fit(X, y)\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    ">>> print(clf.named_steps['linearsvc'].coef_)\n",
    "[[0.141...   0.526... 0.679... 0.493...]]\n",
    "\n",
    ">>> print(clf.named_steps['linearsvc'].intercept_)\n",
    "[0.1693...]\n",
    ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
    "[1]\n",
    "\n",
    "</code>\n",
    "<a href='#top_phases'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f1bbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>1. Library Loading</h3>  <a id='1'></a><small><a href='#top_phases'>back to top</a></small> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(assignment_N, answer):\n",
    "    with open(\"spam_{}.txt\".format(assignment_N), \"w\") as fout:\n",
    "        fout.write(\"{0}\".format(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde9eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = []\n",
    "spam = []\n",
    "\n",
    "with open('SMSSpamCollection.txt','rt') as f:\n",
    "    for line in f.readlines():\n",
    "        cl, message = line.split('\\t')\n",
    "        if cl=='ham':\n",
    "            ham.append(message.strip())\n",
    "        else:\n",
    "            spam.append(message.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18697ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8f235",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb6205",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "texts = ham + spam\n",
    "labels = [0]*len(ham) + [1]*len(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373ff54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>7. Data Preparation | Feature Engineering | Model Building and Training</h3>  <a id='7'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#7'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#7'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#7'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(CountVectorizer(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ed09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, texts, labels, cv=StratifiedKFold(labels, 10,random_state=2), scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6d89c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "write_answer(1, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab831b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>10. Data Preparation | Feature Engineering | Model Building and Training</h3>  <a id='10'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#10'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#10'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#10'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "clf.fit(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "    \"FreeMsg: Txt: claim your reward of 3 hours talk time\",\n",
    "    \"Have you visited the last lecture on physics?\",\n",
    "    \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\",\n",
    "    \"Only 99$\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ba2d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "write_answer(2, \"1 1 0 0 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05506560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>14. Data Preparation | Feature Engineering | Model Building and Training</h3>  <a id='14'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#14'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#14'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#14'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703cd400",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    make_pipeline(CountVectorizer(ngram_range=(2,2)), LogisticRegression()),\n",
    "    make_pipeline(CountVectorizer(ngram_range=(3,3)), LogisticRegression()),\n",
    "    make_pipeline(CountVectorizer(ngram_range=(1,3)), LogisticRegression())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in clfs:\n",
    "    score = cross_val_score(clf, texts, labels, cv=StratifiedKFold(labels, 10,random_state=2), scoring='f1').mean()\n",
    "    print(\"Score: {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e05b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "write_answer(3, '0.82 0.73 0.93')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b2a022",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>17. Library Loading</h3>  <a id='17'></a><small><a href='#top_phases'>back to top</a></small> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a89ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97c925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>18. Data Preparation | Feature Engineering | Model Building and Training</h3>  <a id='18'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#18'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#18'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#18'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9708965",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    make_pipeline(CountVectorizer(ngram_range=(2,2)), MultinomialNB()),\n",
    "    make_pipeline(CountVectorizer(ngram_range=(3,3)), MultinomialNB()),\n",
    "    make_pipeline(CountVectorizer(ngram_range=(1,3)), MultinomialNB())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in clfs:\n",
    "    score = cross_val_score(clf, texts, labels, cv=StratifiedKFold(labels, 10,random_state=2), scoring='f1').mean()\n",
    "    print(\"Score: {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ea9f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "write_answer(4, '0.93 0.87 0.95')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c85896",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>21. Data Preparation | Feature Engineering | Model Building and Training</h3>  <a id='21'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.linear_model._logistic.LogisticRegression</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Logistic Regression (aka logit, MaxEnt) classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
    "scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
    "cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
    "(Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
    "'sag', 'saga' and 'newton-cg' solvers.)\n",
    "\n",
    "This class implements regularized logistic regression using the\n",
    "'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
    "that regularization is applied by default**. It can handle both dense\n",
    "and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
    "floats for optimal performance; any other input format will be converted\n",
    "(and copied).\n",
    "\n",
    "The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
    "with primal formulation, or no regularization. The 'liblinear' solver\n",
    "supports both L1 and L2 regularization, with a dual formulation only for\n",
    "the L2 penalty. The Elastic-Net regularization is only supported by the\n",
    "'saga' solver.\n",
    "\n",
    "Read more in the :ref:`User Guide <logistic_regression>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
    "    Specify the norm of the penalty:\n",
    "\n",
    "    - `None`: no penalty is added;\n",
    "    - `'l2'`: add a L2 penalty term and it is the default choice;\n",
    "    - `'l1'`: add a L1 penalty term;\n",
    "    - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
    "\n",
    "    .. warning::\n",
    "       Some penalties may not work with some solvers. See the parameter\n",
    "       `solver` below, to know the compatibility between the penalty and\n",
    "       solver.\n",
    "\n",
    "    .. versionadded:: 0.19\n",
    "       l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
    "\n",
    "    .. deprecated:: 1.2\n",
    "       The 'none' option was deprecated in version 1.2, and will be removed\n",
    "       in 1.4. Use `None` instead.\n",
    "\n",
    "dual : bool, default=False\n",
    "    Dual or primal formulation. Dual formulation is only implemented for\n",
    "    l2 penalty with liblinear solver. Prefer dual=False when\n",
    "    n_samples > n_features.\n",
    "\n",
    "tol : float, default=1e-4\n",
    "    Tolerance for stopping criteria.\n",
    "\n",
    "C : float, default=1.0\n",
    "    Inverse of regularization strength; must be a positive float.\n",
    "    Like in support vector machines, smaller values specify stronger\n",
    "    regularization.\n",
    "\n",
    "fit_intercept : bool, default=True\n",
    "    Specifies if a constant (a.k.a. bias or intercept) should be\n",
    "    added to the decision function.\n",
    "\n",
    "intercept_scaling : float, default=1\n",
    "    Useful only when the solver 'liblinear' is used\n",
    "    and self.fit_intercept is set to True. In this case, x becomes\n",
    "    [x, self.intercept_scaling],\n",
    "    i.e. a \"synthetic\" feature with constant value equal to\n",
    "    intercept_scaling is appended to the instance vector.\n",
    "    The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
    "\n",
    "    Note! the synthetic feature weight is subject to l1/l2 regularization\n",
    "    as all other features.\n",
    "    To lessen the effect of regularization on synthetic feature weight\n",
    "    (and therefore on the intercept) intercept_scaling has to be increased.\n",
    "\n",
    "class_weight : dict or 'balanced', default=None\n",
    "    Weights associated with classes in the form ``{class_label: weight}``.\n",
    "    If not given, all classes are supposed to have weight one.\n",
    "\n",
    "    The \"balanced\" mode uses the values of y to automatically adjust\n",
    "    weights inversely proportional to class frequencies in the input data\n",
    "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "    Note that these weights will be multiplied with sample_weight (passed\n",
    "    through the fit method) if sample_weight is specified.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *class_weight='balanced'*\n",
    "\n",
    "random_state : int, RandomState instance, default=None\n",
    "    Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
    "    data. See :term:`Glossary <random_state>` for details.\n",
    "\n",
    "solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
    "\n",
    "    Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
    "    To choose a solver, you might want to consider the following aspects:\n",
    "\n",
    "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
    "          and 'saga' are faster for large ones;\n",
    "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
    "          'lbfgs' handle multinomial loss;\n",
    "        - 'liblinear' is limited to one-versus-rest schemes.\n",
    "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
    "          especially with one-hot encoded categorical features with rare\n",
    "          categories. Note that it is limited to binary classification and the\n",
    "          one-versus-rest reduction for multiclass classification. Be aware that\n",
    "          the memory usage of this solver has a quadratic dependency on\n",
    "          `n_features` because it explicitly computes the Hessian matrix.\n",
    "\n",
    "    .. warning::\n",
    "       The choice of the algorithm depends on the penalty chosen.\n",
    "       Supported penalties by solver:\n",
    "\n",
    "       - 'lbfgs'           -   ['l2', None]\n",
    "       - 'liblinear'       -   ['l1', 'l2']\n",
    "       - 'newton-cg'       -   ['l2', None]\n",
    "       - 'newton-cholesky' -   ['l2', None]\n",
    "       - 'sag'             -   ['l2', None]\n",
    "       - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
    "\n",
    "    .. note::\n",
    "       'sag' and 'saga' fast convergence is only guaranteed on features\n",
    "       with approximately the same scale. You can preprocess the data with\n",
    "       a scaler from :mod:`sklearn.preprocessing`.\n",
    "\n",
    "    .. seealso::\n",
    "       Refer to the User Guide for more information regarding\n",
    "       :class:`LogisticRegression` and more specifically the\n",
    "       :ref:`Table <Logistic_regression>`\n",
    "       summarizing solver/penalty supports.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       Stochastic Average Gradient descent solver.\n",
    "    .. versionadded:: 0.19\n",
    "       SAGA solver.\n",
    "    .. versionchanged:: 0.22\n",
    "        The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
    "    .. versionadded:: 1.2\n",
    "       newton-cholesky solver.\n",
    "\n",
    "max_iter : int, default=100\n",
    "    Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
    "    If the option chosen is 'ovr', then a binary problem is fit for each\n",
    "    label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
    "    across the entire probability distribution, *even when the data is\n",
    "    binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
    "    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
    "    and otherwise selects 'multinomial'.\n",
    "\n",
    "    .. versionadded:: 0.18\n",
    "       Stochastic Average Gradient descent solver for 'multinomial' case.\n",
    "    .. versionchanged:: 0.22\n",
    "        Default changed from 'ovr' to 'auto' in 0.22.\n",
    "\n",
    "verbose : int, default=0\n",
    "    For the liblinear and lbfgs solvers set verbose to any positive\n",
    "    number for verbosity.\n",
    "\n",
    "warm_start : bool, default=False\n",
    "    When set to True, reuse the solution of the previous call to fit as\n",
    "    initialization, otherwise, just erase the previous solution.\n",
    "    Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
    "\n",
    "    .. versionadded:: 0.17\n",
    "       *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
    "\n",
    "n_jobs : int, default=None\n",
    "    Number of CPU cores used when parallelizing over classes if\n",
    "    multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
    "    set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
    "    not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "    context. ``-1`` means using all processors.\n",
    "    See :term:`Glossary <n_jobs>` for more details.\n",
    "\n",
    "l1_ratio : float, default=None\n",
    "    The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
    "    used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
    "    to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
    "    to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
    "    combination of L1 and L2.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "\n",
    "classes_ : ndarray of shape (n_classes, )\n",
    "    A list of class labels known to the classifier.\n",
    "\n",
    "coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
    "    Coefficient of the features in the decision function.\n",
    "\n",
    "    `coef_` is of shape (1, n_features) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
    "    to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
    "\n",
    "intercept_ : ndarray of shape (1,) or (n_classes,)\n",
    "    Intercept (a.k.a. bias) added to the decision function.\n",
    "\n",
    "    If `fit_intercept` is set to False, the intercept is set to zero.\n",
    "    `intercept_` is of shape (1,) when the given problem is binary.\n",
    "    In particular, when `multi_class='multinomial'`, `intercept_`\n",
    "    corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
    "    outcome 0 (False).\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
    "    Actual number of iterations for all classes. If binary or multinomial,\n",
    "    it returns only 1 element. For liblinear solver, only the maximum\n",
    "    number of iteration across all classes is given.\n",
    "\n",
    "    .. versionchanged:: 0.20\n",
    "\n",
    "        In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
    "        ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "SGDClassifier : Incrementally trained logistic regression (when given\n",
    "    the parameter ``loss=\"log\"``).\n",
    "LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The underlying C implementation uses a random number generator to\n",
    "select features when fitting the model. It is thus not uncommon,\n",
    "to have slightly different results for the same input data. If\n",
    "that happens, try with a smaller tol parameter.\n",
    "\n",
    "Predict output may not match that of standalone liblinear in certain\n",
    "cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
    "in the narrative documentation.\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
    "    Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
    "    http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
    "\n",
    "LIBLINEAR -- A Library for Large Linear Classification\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
    "\n",
    "SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
    "    Minimizing Finite Sums with the Stochastic Average Gradient\n",
    "    https://hal.inria.fr/hal-00860051/document\n",
    "\n",
    "SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
    "        :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
    "        for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
    "\n",
    "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
    "    methods for logistic regression and maximum entropy models.\n",
    "    Machine Learning 85(1-2):41-75.\n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.linear_model import LogisticRegression\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
    ">>> clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    ">>> clf.predict_proba(X[:2, :])\n",
    "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    ">>> clf.score(X, y)\n",
    "0.97...\n",
    "\n",
    "</code>\n",
    "<a href='#21'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.pipeline.make_pipeline</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Construct a :class:`Pipeline` from the given estimators.\n",
    "\n",
    "This is a shorthand for the :class:`Pipeline` constructor; it does not\n",
    "require, and does not permit, naming the estimators. Instead, their names\n",
    "will be set to the lowercase of their types automatically.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*steps : list of Estimator objects\n",
    "    List of the scikit-learn estimators that are chained together.\n",
    "\n",
    "memory : str or object with the joblib.Memory interface, default=None\n",
    "    Used to cache the fitted transformers of the pipeline. By default,\n",
    "    no caching is performed. If a string is given, it is the path to\n",
    "    the caching directory. Enabling caching triggers a clone of\n",
    "    the transformers before fitting. Therefore, the transformer\n",
    "    instance given to the pipeline cannot be inspected\n",
    "    directly. Use the attribute ``named_steps`` or ``steps`` to\n",
    "    inspect estimators within the pipeline. Caching the\n",
    "    transformers is advantageous when fitting is time consuming.\n",
    "\n",
    "verbose : bool, default=False\n",
    "    If True, the time elapsed while fitting each step will be printed as it\n",
    "    is completed.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "p : Pipeline\n",
    "    Returns a scikit-learn :class:`Pipeline` object.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "Pipeline : Class for creating a pipeline of transforms with a final\n",
    "    estimator.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> from sklearn.preprocessing import StandardScaler\n",
    ">>> from sklearn.pipeline import make_pipeline\n",
    ">>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n",
    "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
    "                ('gaussiannb', GaussianNB())])\n",
    "\n",
    "</code>\n",
    "<a href='#21'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.TfidfVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to :class:`CountVectorizer` followed by\n",
    ":class:`TfidfTransformer`.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (string transformation) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word or character n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
    "        is first read from the file and then passed to the given callable\n",
    "        analyzer.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
    "    list is returned. 'english' is currently the only supported string\n",
    "    value.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
    "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
    "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
    "    only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "max_df : float or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
    "    documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
    "    of documents, integer absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non-zero term counts are set to 1. This does not mean\n",
    "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
    "    is binary. (Set idf and normalization to False to get 0/1 outputs).\n",
    "\n",
    "dtype : dtype, default=float64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "norm : {'l1', 'l2'} or None, default='l2'\n",
    "    Each output row will have unit norm, either:\n",
    "\n",
    "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
    "      similarity between two vectors is their dot product when l2 norm has\n",
    "      been applied.\n",
    "    - 'l1': Sum of absolute values of vector elements is 1.\n",
    "      See :func:`preprocessing.normalize`.\n",
    "    - None: No normalization.\n",
    "\n",
    "use_idf : bool, default=True\n",
    "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
    "\n",
    "smooth_idf : bool, default=True\n",
    "    Smooth idf weights by adding one to document frequencies, as if an\n",
    "    extra document was seen containing every term in the collection\n",
    "    exactly once. Prevents zero divisions.\n",
    "\n",
    "sublinear_tf : bool, default=False\n",
    "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "idf_ : array of shape (n_features,)\n",
    "    The inverse document frequency (IDF) vector; only defined\n",
    "    if ``use_idf`` is True.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
    "\n",
    "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
    "    matrix of counts.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = TfidfVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.shape)\n",
    "(4, 9)\n",
    "\n",
    "</code>\n",
    "<a href='#21'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "cross_val_score(clf, texts, labels, cv=StratifiedKFold(labels, 10,random_state=2), scoring='f1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4964f3d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "write_answer(5, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e0719",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "### Fix for question 4\n",
    "### Answer is wrong, when using make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030bac2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "lines_to_next_cell": 1,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>24. Feature Engineering</h3>  <a id='24'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#24'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#24'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e4a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(ngram_range, clf):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    data = vectorizer.fit_transform(texts)\n",
    "    return cross_val_score(clf, data, labels, cv=StratifiedKFold(labels, 10,random_state=2), scoring='f1').mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f333f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<div> <h3 class='hg'>25. Feature Engineering | Model Building and Training</h3>  <a id='25'></a><small><a href='#top_phases'>back to top</a></small><details><summary style='list-style: none; cursor: pointer;'><u>View function calls</u></summary>\n",
    "<ul>\n",
    "\n",
    "<li> <strong class='hglib'>sklearn</strong>\n",
    "<ul>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer.fit_transform</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Learn the vocabulary dictionary and return document-term matrix.\n",
    "\n",
    "This is equivalent to fit followed by transform, but more efficiently\n",
    "implemented.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "raw_documents : iterable\n",
    "    An iterable which generates either str, unicode or file objects.\n",
    "\n",
    "y : None\n",
    "    This parameter is ignored.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "X : array of shape (n_samples, n_features)\n",
    "    Document-term matrix.\n",
    "\n",
    "</code>\n",
    "<a href='#25'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.feature_extraction.text.CountVectorizer</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Convert a collection of text documents to a matrix of token counts.\n",
    "\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
    "that does some kind of feature selection then the number of features will\n",
    "be equal to the vocabulary size found by analyzing the data.\n",
    "\n",
    "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "input : {'filename', 'file', 'content'}, default='content'\n",
    "    - If `'filename'`, the sequence passed as an argument to fit is\n",
    "      expected to be a list of filenames that need reading to fetch\n",
    "      the raw content to analyze.\n",
    "\n",
    "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
    "      object) that is called to fetch the bytes in memory.\n",
    "\n",
    "    - If `'content'`, the input is expected to be a sequence of items that\n",
    "      can be of type string or byte.\n",
    "\n",
    "encoding : str, default='utf-8'\n",
    "    If bytes or files are given to analyze, this encoding is used to\n",
    "    decode.\n",
    "\n",
    "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
    "    Instruction on what to do if a byte sequence is given to analyze that\n",
    "    contains characters not of the given `encoding`. By default, it is\n",
    "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
    "    values are 'ignore' and 'replace'.\n",
    "\n",
    "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
    "    Remove accents and perform other character normalization\n",
    "    during the preprocessing step.\n",
    "    'ascii' is a fast method that only works on characters that have\n",
    "    a direct ASCII mapping.\n",
    "    'unicode' is a slightly slower method that works on any characters.\n",
    "    None (default) does nothing.\n",
    "\n",
    "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
    "    :func:`unicodedata.normalize`.\n",
    "\n",
    "lowercase : bool, default=True\n",
    "    Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "preprocessor : callable, default=None\n",
    "    Override the preprocessing (strip_accents and lowercase) stage while\n",
    "    preserving the tokenizing and n-grams generation steps.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "tokenizer : callable, default=None\n",
    "    Override the string tokenization step while preserving the\n",
    "    preprocessing and n-grams generation steps.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "stop_words : {'english'}, list, default=None\n",
    "    If 'english', a built-in stop word list for English is used.\n",
    "    There are several known issues with 'english' and you should\n",
    "    consider an alternative (see :ref:`stop_words`).\n",
    "\n",
    "    If a list, that list is assumed to contain stop words, all of which\n",
    "    will be removed from the resulting tokens.\n",
    "    Only applies if ``analyzer == 'word'``.\n",
    "\n",
    "    If None, no stop words will be used. max_df can be set to a value\n",
    "    in the range [0.7, 1.0) to automatically detect and filter stop\n",
    "    words based on intra corpus document frequency of terms.\n",
    "\n",
    "token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
    "    Regular expression denoting what constitutes a \"token\", only used\n",
    "    if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
    "    or more alphanumeric characters (punctuation is completely ignored\n",
    "    and always treated as a token separator).\n",
    "\n",
    "    If there is a capturing group in token_pattern then the\n",
    "    captured group content, not the entire match, becomes the token.\n",
    "    At most one capturing group is permitted.\n",
    "\n",
    "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
    "    The lower and upper boundary of the range of n-values for different\n",
    "    word n-grams or char n-grams to be extracted. All values of n such\n",
    "    such that min_n <= n <= max_n will be used. For example an\n",
    "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
    "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
    "    Only applies if ``analyzer`` is not callable.\n",
    "\n",
    "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
    "    Whether the feature should be made of word n-gram or character\n",
    "    n-grams.\n",
    "    Option 'char_wb' creates character n-grams only from text inside\n",
    "    word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "    If a callable is passed it is used to extract the sequence of features\n",
    "    out of the raw, unprocessed input.\n",
    "\n",
    "    .. versionchanged:: 0.21\n",
    "\n",
    "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
    "    first read from the file and then passed to the given callable\n",
    "    analyzer.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "max_features : int, default=None\n",
    "    If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "vocabulary : Mapping or iterable, default=None\n",
    "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
    "    indices in the feature matrix, or an iterable over terms. If not\n",
    "    given, a vocabulary is determined from the input documents. Indices\n",
    "    in the mapping should not be repeated and should not have any gap\n",
    "    between 0 and the largest index.\n",
    "\n",
    "binary : bool, default=False\n",
    "    If True, all non zero counts are set to 1. This is useful for discrete\n",
    "    probabilistic models that model binary events rather than integer\n",
    "    counts.\n",
    "\n",
    "dtype : dtype, default=np.int64\n",
    "    Type of the matrix returned by fit_transform() or transform().\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "vocabulary_ : dict\n",
    "    A mapping of terms to feature indices.\n",
    "\n",
    "fixed_vocabulary_ : bool\n",
    "    True if a fixed vocabulary of term to indices mapping\n",
    "    is provided by the user.\n",
    "\n",
    "stop_words_ : set\n",
    "    Terms that were ignored because they either:\n",
    "\n",
    "      - occurred in too many documents (`max_df`)\n",
    "      - occurred in too few documents (`min_df`)\n",
    "      - were cut off by feature selection (`max_features`).\n",
    "\n",
    "    This is only available if no vocabulary was given.\n",
    "\n",
    "See Also\n",
    "--------\n",
    "HashingVectorizer : Convert a collection of text documents to a\n",
    "    matrix of token counts.\n",
    "\n",
    "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
    "    of TF-IDF features.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "The ``stop_words_`` attribute can get large and increase the model size\n",
    "when pickling. This attribute is provided only for introspection and can\n",
    "be safely removed using delattr or set to None before pickling.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    ">>> vectorizer = CountVectorizer()\n",
    ">>> X = vectorizer.fit_transform(corpus)\n",
    ">>> vectorizer.get_feature_names_out()\n",
    "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
    "       'this'], ...)\n",
    ">>> print(X.toarray())\n",
    "[[0 1 1 1 0 0 1 0 1]\n",
    " [0 2 0 1 0 1 1 0 1]\n",
    " [1 0 0 1 1 0 1 1 1]\n",
    " [0 1 1 1 0 0 1 0 1]]\n",
    ">>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    ">>> X2 = vectorizer2.fit_transform(corpus)\n",
    ">>> vectorizer2.get_feature_names_out()\n",
    "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
    "       'second document', 'the first', 'the second', 'the third', 'third one',\n",
    "       'this document', 'this is', 'this the'], ...)\n",
    " >>> print(X2.toarray())\n",
    " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
    " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
    " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
    " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
    "\n",
    "</code>\n",
    "<a href='#25'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "<li>\n",
    "<details><summary style='list-style: none; cursor: pointer;'><u>sklearn.naive_bayes.MultinomialNB</u></summary>\n",
    "<blockquote>\n",
    "<code>\n",
    "Naive Bayes classifier for multinomial models.\n",
    "\n",
    "The multinomial Naive Bayes classifier is suitable for classification with\n",
    "discrete features (e.g., word counts for text classification). The\n",
    "multinomial distribution normally requires integer feature counts. However,\n",
    "in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "alpha : float or array-like of shape (n_features,), default=1.0\n",
    "    Additive (Laplace/Lidstone) smoothing parameter\n",
    "    (set alpha=0 and force_alpha=True, for no smoothing).\n",
    "\n",
    "force_alpha : bool, default=False\n",
    "    If False and alpha is less than 1e-10, it will set alpha to\n",
    "    1e-10. If True, alpha will remain unchanged. This may cause\n",
    "    numerical errors if alpha is too close to 0.\n",
    "\n",
    "    .. versionadded:: 1.2\n",
    "    .. deprecated:: 1.2\n",
    "       The default value of `force_alpha` will change to `True` in v1.4.\n",
    "\n",
    "fit_prior : bool, default=True\n",
    "    Whether to learn class prior probabilities or not.\n",
    "    If false, a uniform prior will be used.\n",
    "\n",
    "class_prior : array-like of shape (n_classes,), default=None\n",
    "    Prior probabilities of the classes. If specified, the priors are not\n",
    "    adjusted according to the data.\n",
    "\n",
    "Attributes\n",
    "----------\n",
    "class_count_ : ndarray of shape (n_classes,)\n",
    "    Number of samples encountered for each class during fitting. This\n",
    "    value is weighted by the sample weight when provided.\n",
    "\n",
    "class_log_prior_ : ndarray of shape (n_classes,)\n",
    "    Smoothed empirical log probability for each class.\n",
    "\n",
    "classes_ : ndarray of shape (n_classes,)\n",
    "    Class labels known to the classifier\n",
    "\n",
    "feature_count_ : ndarray of shape (n_classes, n_features)\n",
    "    Number of samples encountered for each (class, feature)\n",
    "    during fitting. This value is weighted by the sample weight when\n",
    "    provided.\n",
    "\n",
    "feature_log_prob_ : ndarray of shape (n_classes, n_features)\n",
    "    Empirical log probability of features\n",
    "    given a class, ``P(x_i|y)``.\n",
    "\n",
    "n_features_in_ : int\n",
    "    Number of features seen during :term:`fit`.\n",
    "\n",
    "    .. versionadded:: 0.24\n",
    "\n",
    "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "    Names of features seen during :term:`fit`. Defined only when `X`\n",
    "    has feature names that are all strings.\n",
    "\n",
    "    .. versionadded:: 1.0\n",
    "\n",
    "See Also\n",
    "--------\n",
    "BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "CategoricalNB : Naive Bayes classifier for categorical features.\n",
    "ComplementNB : Complement Naive Bayes classifier.\n",
    "GaussianNB : Gaussian Naive Bayes.\n",
    "\n",
    "References\n",
    "----------\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> import numpy as np\n",
    ">>> rng = np.random.RandomState(1)\n",
    ">>> X = rng.randint(5, size=(6, 100))\n",
    ">>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    ">>> from sklearn.naive_bayes import MultinomialNB\n",
    ">>> clf = MultinomialNB(force_alpha=True)\n",
    ">>> clf.fit(X, y)\n",
    "MultinomialNB(force_alpha=True)\n",
    ">>> print(clf.predict(X[2:3]))\n",
    "[3]\n",
    "\n",
    "</code>\n",
    "<a href='#25'>back to header</a>\n",
    "</blockquote>\n",
    "</details>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "\n",
    "</ul>\n",
    "</details> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_params = [(2,2),(3,3),(1,3)]\n",
    "for param in ngram_params:\n",
    "    score=get_score(param, MultinomialNB())\n",
    "    print(\"Score: {0:.2f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd5cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(4, '0.65 0.38 0.89')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_env]",
   "language": "python",
   "name": "conda-env-py3_env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
